{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2497a87d-4829-4692-b1c7-96430a78c091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.2885\n",
      "Epoch 100: Loss = 0.2466\n",
      "Epoch 200: Loss = 0.2442\n",
      "Epoch 300: Loss = 0.2416\n",
      "Epoch 400: Loss = 0.2388\n",
      "Epoch 500: Loss = 0.2355\n",
      "Epoch 600: Loss = 0.2315\n",
      "Epoch 700: Loss = 0.2267\n",
      "Epoch 800: Loss = 0.2208\n",
      "Epoch 900: Loss = 0.2137\n",
      "Predictions:\n",
      "[[0.40164534]\n",
      " [0.4589818 ]\n",
      " [0.62812302]\n",
      " [0.47848862]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.bias_input_hidden = np.random.randn(1, self.hidden_size)\n",
    "        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)\n",
    "        self.bias_hidden_output = np.random.randn(1, self.output_size)\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Input to hidden layer\n",
    "        self.hidden_output = self.sigmoid(np.dot(X, self.weights_input_hidden) + self.bias_input_hidden)\n",
    "        # Hidden to output layer\n",
    "        self.predicted_output = self.sigmoid(np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_hidden_output)\n",
    "        return self.predicted_output\n",
    "    \n",
    "    def backward(self, X, y, learning_rate):\n",
    "        # Calculate error\n",
    "        error = y - self.predicted_output\n",
    "        \n",
    "        # Compute gradients\n",
    "        delta_output = error * self.sigmoid_derivative(self.predicted_output)\n",
    "        delta_hidden = delta_output.dot(self.weights_hidden_output.T) * self.sigmoid_derivative(self.hidden_output)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.weights_hidden_output += self.hidden_output.T.dot(delta_output) * learning_rate\n",
    "        self.bias_hidden_output += np.sum(delta_output, axis=0, keepdims=True) * learning_rate\n",
    "        self.weights_input_hidden += X.T.dot(delta_hidden) * learning_rate\n",
    "        self.bias_input_hidden += np.sum(delta_hidden, axis=0, keepdims=True) * learning_rate\n",
    "    \n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            self.backward(X, y, learning_rate)\n",
    "            if epoch % 100 == 0:\n",
    "                loss = np.mean(np.square(y - output))\n",
    "                print(f'Epoch {epoch}: Loss = {loss:.4f}')\n",
    "                \n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "# Example usage\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "input_size = 2\n",
    "hidden_size = 3\n",
    "output_size = 1\n",
    "\n",
    "nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "nn.train(X, y, epochs=1000, learning_rate=0.1)\n",
    "\n",
    "# Test the trained model\n",
    "print(\"Predictions:\")\n",
    "print(nn.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd45e66d-2152-428e-90a0-ef607f37aa09",
   "metadata": {},
   "source": [
    "The code defines a simple feedforward neural network with one hidden layer and a single output. The network uses the sigmoid activation function for both hidden and output layers, and it trains with a basic form of backpropagation. Let's break it down step-by-step.\n",
    "\n",
    "### Class Definition\n",
    "- `class NeuralNetwork:`: Defines the Neural Network class.\n",
    "\n",
    "### Initialization (`__init__`)\n",
    "- `def __init__(self, input_size, hidden_size, output_size):`: Constructor that initializes the neural network with specified input, hidden, and output layer sizes.\n",
    "- `self.input_size = input_size`: Stores the input layer size.\n",
    "- `self.hidden_size = hidden_size`: Stores the hidden layer size.\n",
    "- `self.output_size = output_size`: Stores the output layer size.\n",
    "- **Weights and Biases Initialization**:\n",
    "  - `self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)`: Initializes the weights between the input and hidden layers with random values from a standard normal distribution.\n",
    "  - `self.bias_input_hidden = np.random.randn(1, self.hidden_size)`: Initializes the biases for the hidden layer.\n",
    "  - `self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)`: Initializes the weights between the hidden and output layers.\n",
    "  - `self.bias_hidden_output = np.random.randn(1, self.output_size)`: Initializes the biases for the output layer.\n",
    "\n",
    "### Activation Functions\n",
    "- `def sigmoid(self, x):`: Defines the sigmoid activation function, which maps any real number to a value between 0 and 1.\n",
    "  - `return 1 / (1 + np.exp(-x))`: The sigmoid function formula.\n",
    "- `def sigmoid_derivative(self, x):`: Defines the derivative of the sigmoid function, used in backpropagation to compute gradients.\n",
    "  - `return x * (1 - x)`: The derivative of sigmoid. This derivative helps determine how much to adjust the weights during training.\n",
    "\n",
    "### Forward Propagation (`forward`)\n",
    "- `def forward(self, X):`: Defines the forward propagation method, which computes the outputs of the network given an input `X`.\n",
    "  - `self.hidden_output = self.sigmoid(np.dot(X, self.weights_input_hidden) + self.bias_input_hidden)`: Computes the hidden layer output by taking the dot product of `X` with the weights for the input-to-hidden layer, adding the bias, then applying the sigmoid function.\n",
    "  - `self.predicted_output = self.sigmoid(np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_hidden_output)`: Computes the output layer by doing a similar operation, using the hidden layer's output.\n",
    "  - `return self.predicted_output`: Returns the predicted output from the network.\n",
    "\n",
    "### Backward Propagation (`backward`)\n",
    "- `def backward(self, X, y, learning_rate):`: Defines the backward propagation method to update the weights and biases based on the error between the predicted and actual output.\n",
    "  - `error = y - self.predicted_output`: Computes the error between the actual output `y` and the predicted output `self.predicted_output`.\n",
    "  - **Calculate Gradients**:\n",
    "    - `delta_output = error * self.sigmoid_derivative(self.predicted_output)`: Calculates the gradient for the output layer using the error and the derivative of sigmoid.\n",
    "    - `delta_hidden = delta_output.dot(self.weights_hidden_output.T) * self.sigmoid_derivative(self.hidden_output)`: Computes the gradient for the hidden layer by propagating the `delta_output` backward through the weights, then multiplying by the derivative of sigmoid.\n",
    "  - **Update Weights and Biases**:\n",
    "    - `self.weights_hidden_output += self.hidden_output.T.dot(delta_output) * learning_rate`: Updates the weights between hidden and output layers using the gradients and the learning rate.\n",
    "    - `self.bias_hidden_output += np.sum(delta_output, axis=0, keepdims=True) * learning_rate`: Updates the bias for the output layer.\n",
    "    - `self.weights_input_hidden += X.T.dot(delta_hidden) * learning_rate`: Updates the weights between input and hidden layers.\n",
    "    - `self.bias_input_hidden += np.sum(delta_hidden, axis=0, keepdims=True) * learning_rate`: Updates the bias for the hidden layer.\n",
    "\n",
    "### Training (`train`)\n",
    "- `def train(self, X, y, epochs, learning_rate):`: Defines the training method.\n",
    "  - `for epoch in range(epochs)`: Loops over the specified number of epochs.\n",
    "  - `output = self.forward(X)`: Performs forward propagation to compute the network's output given the training data `X`.\n",
    "  - `self.backward(X, y, learning_rate)`: Applies backward propagation to update weights and biases.\n",
    "  - `if epoch % 100 == 0:`: Every 100 epochs, a checkpoint for printing the loss.\n",
    "    - `loss = np.mean(np.square(y - output))`: Computes the mean squared error loss.\n",
    "    - `print(f'Epoch {epoch}: Loss = {loss:.4f}')`: Outputs the loss for monitoring training progress.\n",
    "\n",
    "### Prediction (`predict`)\n",
    "- `def predict(self, X):`: Defines the predict method to generate predictions for given input `X`.\n",
    "  - `return self.forward(X)`: Returns the predicted output after forward propagation.\n",
    "\n",
    "### Example Usage\n",
    "- **Create Input Data**:\n",
    "  - `X = np.array([[0,0], [0,1], [1,0], [1,1]])`: Example data for training, representing the inputs for an XOR operation.\n",
    "  - `y = np.array([[0], [1], [1], [0]])`: The corresponding expected outputs (target values).\n",
    "- **Initialize Neural Network**:\n",
    "  - `nn = NeuralNetwork(input_size, hidden_size, output_size)`: Creates an instance of the neural network with specified sizes.\n",
    "- **Train the Neural Network**:\n",
    "  - `nn.train(X, y, epochs=1000, learning_rate=0.1)`: Trains the network on the given data for 1000 epochs with a learning rate of 0.1.\n",
    "- **Test the Trained Model**:\n",
    "  - `print(\"Predictions:\")`: Outputs the heading for the predictions.\n",
    "  - `print(nn.predict(X))`: Makes predictions for the given inputs and prints the results.\n",
    "\n",
    "### Summary\n",
    "This code creates a simple feedforward neural network with one hidden layer and uses backpropagation to update weights and biases. The network is trained on an XOR-like dataset and tested to see how well it performs after training. The code demonstrates fundamental concepts in neural networks, including activation functions, backpropagation, and training with epochs and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28190f9-4166-4bcc-8c91-f6a63689b79f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
