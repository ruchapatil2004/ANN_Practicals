{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47ec040c-57e5-410c-b284-1ddef3f63292",
   "metadata": {},
   "source": [
    "Tenserflow and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97f06145-e31f-42ad-a807-24eac25b5b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 6us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8577 - loss: 0.4912\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9538 - loss: 0.1555\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9675 - loss: 0.1111\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9728 - loss: 0.0887\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9771 - loss: 0.0750\n",
      "313/313 - 1s - 2ms/step - accuracy: 0.9772 - loss: 0.0780\n",
      "Test accuracy: 0.9771999716758728\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers, models, datasets\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Load and Preprocess Data\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
    "\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Define the Neural Network Architecture\n",
    "\n",
    "model = models.Sequential([\n",
    "\n",
    "    layers.Flatten(input_shape=(28, 28)),\n",
    "\n",
    "    layers.Dense(128, activation='relu'),\n",
    "\n",
    "    layers.Dropout(0.2),\n",
    "\n",
    "    layers.Dense(10)\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Compile the Model\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Train the Model\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=5)\n",
    "\n",
    "\n",
    "\n",
    "# Step 5: Evaluate the Model\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "\n",
    "print(f'Test accuracy: {test_acc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81154c19-a48b-4757-9ae2-757cadc28b1c",
   "metadata": {},
   "source": [
    "Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4ad5b7f-9fd7-4136-8007-135017293e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 9912422/9912422 [00:18<00:00, 544907.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 28881/28881 [00:00<00:00, 117390.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1648877/1648877 [00:07<00:00, 211343.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 4542/4542 [00:00<00:00, 4591595.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Epoch: 1, Batch: 100, Loss: 2.2529763865470884\n",
      "Epoch: 1, Batch: 200, Loss: 2.052003548145294\n",
      "Epoch: 1, Batch: 300, Loss: 1.4005225563049317\n",
      "Epoch: 1, Batch: 400, Loss: 0.7444566258788109\n",
      "Epoch: 1, Batch: 500, Loss: 0.49590682715177536\n",
      "Epoch: 1, Batch: 600, Loss: 0.402982544451952\n",
      "Epoch: 1, Batch: 700, Loss: 0.3645714870095253\n",
      "Epoch: 1, Batch: 800, Loss: 0.31477760449051856\n",
      "Epoch: 1, Batch: 900, Loss: 0.28590532541275027\n",
      "Epoch: 2, Batch: 100, Loss: 0.2620733288675547\n",
      "Epoch: 2, Batch: 200, Loss: 0.2306802911311388\n",
      "Epoch: 2, Batch: 300, Loss: 0.23794006183743477\n",
      "Epoch: 2, Batch: 400, Loss: 0.2216922351717949\n",
      "Epoch: 2, Batch: 500, Loss: 0.19419811762869357\n",
      "Epoch: 2, Batch: 600, Loss: 0.17479369558393956\n",
      "Epoch: 2, Batch: 700, Loss: 0.1830508702993393\n",
      "Epoch: 2, Batch: 800, Loss: 0.1671824785321951\n",
      "Epoch: 2, Batch: 900, Loss: 0.1569905024021864\n",
      "Epoch: 3, Batch: 100, Loss: 0.14940424928441642\n",
      "Epoch: 3, Batch: 200, Loss: 0.1429625246860087\n",
      "Epoch: 3, Batch: 300, Loss: 0.13471671096980573\n",
      "Epoch: 3, Batch: 400, Loss: 0.12811067448928953\n",
      "Epoch: 3, Batch: 500, Loss: 0.13251211553812026\n",
      "Epoch: 3, Batch: 600, Loss: 0.13083265095949173\n",
      "Epoch: 3, Batch: 700, Loss: 0.11522036485373974\n",
      "Epoch: 3, Batch: 800, Loss: 0.11863193752244115\n",
      "Epoch: 3, Batch: 900, Loss: 0.1223632682301104\n",
      "Epoch: 4, Batch: 100, Loss: 0.11383635895326734\n",
      "Epoch: 4, Batch: 200, Loss: 0.10854240899905562\n",
      "Epoch: 4, Batch: 300, Loss: 0.10606964603066445\n",
      "Epoch: 4, Batch: 400, Loss: 0.102695480780676\n",
      "Epoch: 4, Batch: 500, Loss: 0.10001253545284271\n",
      "Epoch: 4, Batch: 600, Loss: 0.10272482199594378\n",
      "Epoch: 4, Batch: 700, Loss: 0.09302641204558312\n",
      "Epoch: 4, Batch: 800, Loss: 0.09042833908461034\n",
      "Epoch: 4, Batch: 900, Loss: 0.0838735192315653\n",
      "Epoch: 5, Batch: 100, Loss: 0.09259789815172553\n",
      "Epoch: 5, Batch: 200, Loss: 0.09068516669794917\n",
      "Epoch: 5, Batch: 300, Loss: 0.08349061496555805\n",
      "Epoch: 5, Batch: 400, Loss: 0.08359867990016938\n",
      "Epoch: 5, Batch: 500, Loss: 0.08155202890746295\n",
      "Epoch: 5, Batch: 600, Loss: 0.07991495527327061\n",
      "Epoch: 5, Batch: 700, Loss: 0.08037386529147625\n",
      "Epoch: 5, Batch: 800, Loss: 0.07441946471109986\n",
      "Epoch: 5, Batch: 900, Loss: 0.07940690504387021\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Load and Preprocess Data\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Define the Neural Network Architecture\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "\n",
    "        self.fc1 = nn.Linear(1600, 128)\n",
    "\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.relu(self.conv1(x))\n",
    "\n",
    "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
    "\n",
    "        x = torch.relu(self.conv2(x))\n",
    "\n",
    "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
    "\n",
    "        x = x.view(-1, 1600)\n",
    "\n",
    "        x = torch.relu(self.fc1(x))\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "model = Net()\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Define Loss Function and Optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Train the Model\n",
    "\n",
    "for epoch in range(5):\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 100 == 99:\n",
    "\n",
    "            print(f'Epoch: {epoch + 1}, Batch: {i + 1}, Loss: {running_loss / 100}')\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3930f22-0ab8-4451-be7f-30246a2b396c",
   "metadata": {},
   "source": [
    "This code snippet demonstrates how to create and train neural networks to classify digits from the MNIST dataset, using both TensorFlow/Keras and PyTorch frameworks. Here's an explanation of each step for both implementations.\n",
    "\n",
    "### TensorFlow/Keras Implementation\n",
    "#### Step 1: Load and Preprocess Data\n",
    "- `import tensorflow as tf`: Imports TensorFlow for building and training neural networks.\n",
    "- `from tensorflow.keras import layers, models, datasets`: Imports specific modules for defining layers, models, and datasets.\n",
    "- `datasets.mnist.load_data()`: Loads the MNIST dataset, containing 60,000 training images and 10,000 test images, with 28x28 grayscale images of handwritten digits.\n",
    "- `train_images, train_labels, test_images, test_labels`: Assigns the loaded training and testing images and labels.\n",
    "- `train_images, test_images = train_images / 255.0, test_images / 255.0`: Normalizes pixel values to be in the range [0, 1] by dividing by 255.0, making training more stable.\n",
    "\n",
    "#### Step 2: Define the Neural Network Architecture\n",
    "- `model = models.Sequential([ ... ])`: Creates a sequential model, where layers are added in sequence.\n",
    "  - `layers.Flatten(input_shape=(28, 28))`: Flattens the 28x28 image into a 1D array of 784 elements.\n",
    "  - `layers.Dense(128, activation='relu')`: Adds a dense (fully connected) layer with 128 units, using the ReLU activation function.\n",
    "  - `layers.Dropout(0.2)`: Adds a dropout layer that randomly deactivates 20% of the neurons during training, reducing overfitting.\n",
    "  - `layers.Dense(10)`: Adds an output dense layer with 10 units (one for each digit 0-9), representing the final output layer.\n",
    "\n",
    "#### Step 3: Compile the Model\n",
    "- `model.compile(...)`: Compiles the model with specified configurations:\n",
    "  - `optimizer='adam'`: Uses the Adam optimizer for training, which is robust and adaptive.\n",
    "  - `loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)`: Uses sparse categorical cross-entropy as the loss function because the output layer returns raw logits (not probabilities).\n",
    "  - `metrics=['accuracy']`: Tracks accuracy during training.\n",
    "\n",
    "#### Step 4: Train the Model\n",
    "- `model.fit(train_images, train_labels, epochs=5)`: Trains the model on the training data for 5 epochs.\n",
    "\n",
    "#### Step 5: Evaluate the Model\n",
    "- `model.evaluate(test_images, test_labels, verbose=2)`: Evaluates the model on the test data to get the loss and accuracy. `verbose=2` indicates a moderate level of output detail.\n",
    "- `print(f'Test accuracy: {test_acc}')`: Prints the test accuracy, indicating the model's performance on the test data.\n",
    "\n",
    "### PyTorch Implementation\n",
    "#### Step 1: Load and Preprocess Data\n",
    "- `import torch, torchvision, torch.nn as nn, torch.optim as optim, torchvision.transforms as transforms`: Imports PyTorch libraries and utilities for neural networks, optimizers, and data processing.\n",
    "- `transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])`: Defines a transformation to convert images to tensors and normalize them to mean 0.5 and standard deviation 0.5.\n",
    "- `train_set = torchvision.datasets.MNIST(...)`: Loads the MNIST dataset for training, applying the specified `transform`.\n",
    "- `train_loader = torch.utils.data.DataLoader(...)`: Creates a data loader to batch and shuffle the training data. This enables efficient data loading and batching.\n",
    "\n",
    "#### Step 2: Define the Neural Network Architecture\n",
    "- `class Net(nn.Module):`: Defines a neural network class inheriting from `nn.Module`.\n",
    "  - `__init__()`: Constructor to initialize the network architecture.\n",
    "    - `self.conv1 = nn.Conv2d(1, 32, kernel_size=3)`: Adds a 2D convolution layer with 32 filters and a 3x3 kernel size, with 1 input channel (grayscale).\n",
    "    - `self.conv2 = nn.Conv2d(32, 64, kernel_size=3)`: Adds a second 2D convolution layer with 64 filters.\n",
    "    - `self.fc1 = nn.Linear(1600, 128)`: Adds a fully connected (dense) layer with 128 units, receiving a flattened input of 1600 elements.\n",
    "    - `self.fc2 = nn.Linear(128, 10)`: Adds a final fully connected layer with 10 output units, representing the 10 digits.\n",
    "  - `forward(self, x)`: Defines the forward propagation through the network.\n",
    "    - `x = torch.relu(self.conv1(x))`: Applies ReLU activation after the first convolution.\n",
    "    - `x = torch.max_pool2d(x, kernel_size=2, stride=2)`: Applies a 2x2 max-pooling operation to reduce the spatial dimensions.\n",
    "    - `x = torch.relu(self.conv2(x))`: Applies ReLU activation after the second convolution.\n",
    "    - `x = torch.max_pool2d(x, kernel_size=2, stride=2)`: Applies max-pooling again.\n",
    "    - `x = x.view(-1, 1600)`: Flattens the tensor to prepare for fully connected layers.\n",
    "    - `x = torch.relu(self.fc1(x))`: Applies ReLU activation in the first fully connected layer.\n",
    "    - `x = self.fc2(x)`: The final output layer, returning the raw logits.\n",
    "- `model = Net()`: Creates an instance of the defined neural network.\n",
    "\n",
    "#### Step 3: Define Loss Function and Optimizer\n",
    "- `criterion = nn.CrossEntropyLoss()`: Defines the cross-entropy loss function for multi-class classification.\n",
    "- `optimizer = optim.SGD(model.parameters(), lr=0.01)`: Sets up the Stochastic Gradient Descent (SGD) optimizer with a learning rate of 0.01 to update the network's parameters during training.\n",
    "\n",
    "#### Step 4: Train the Model\n",
    "- `for epoch in range(5):`: Runs the training loop for 5 epochs.\n",
    "  - `running_loss = 0.0`: Initializes a variable to accumulate loss for reporting.\n",
    "  - `for i, data in enumerate(train_loader, 0):`: Loops through the training data in batches.\n",
    "    - `inputs, labels = data`: Unpacks the batch into `inputs` (images) and `labels` (corresponding targets).\n",
    "    - `optimizer.zero_grad()`: Resets the gradients to zero.\n",
    "    - `outputs = model(inputs)`: Forwards the batch through the network to get predictions.\n",
    "    - `loss = criterion(outputs, labels)`: Calculates the loss based on the predictions and the labels.\n",
    "    - `loss.backward()`: Performs backpropagation to compute gradients.\n",
    "    - `optimizer.step()`: Updates the model's parameters based on the computed gradients.\n",
    "    - `running_loss += loss.item()`: Adds the loss for this batch to `running_loss`.\n",
    "    - `if i % 100 == 99:`: Outputs progress every 100 batches.\n",
    "      - `print(f'Epoch: {epoch + 1}, Batch: {i + 1}, Loss: {running_loss / 100}')`: Prints the epoch, batch number, and average loss for the last 100 batches.\n",
    "      - `running_loss = 0.0`: Resets `running_loss` for the next set of batches.\n",
    "\n",
    "#### Step 5: Print Training Completion\n",
    "- `print('Finished Training')`: Prints a message indicating the training is complete.\n",
    "\n",
    "### Summary\n",
    "This code snippet demonstrates the implementation and training of simple neural networks on the MNIST dataset using both TensorFlow/Keras and PyTorch. The code includes data loading and preprocessing, defining neural network architectures, compiling/training, and evaluating the model. It shows how to work with two popular deep learning frameworks to achieve similar outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87e35d1-c7ff-4266-9e4d-6eae6282c86b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
